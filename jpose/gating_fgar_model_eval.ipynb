{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 57857 samples, validate on 4246 samples\n",
      "loaded language embeddings\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torchvision.models as models\n",
    "from resnext_specialist import VA\n",
    "from data_cnn60 import NTUDataLoaders, AverageMeter, make_dir, get_cases, get_num_classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from fgar import AE, PosMmen, JPosMmen, contractive_loss, MMDLoss, multi_class_hinge_loss, triplet_loss, Mmen\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='View adaptive')\n",
    "# parser.add_argument('--ss', type=int, help=\"split size\")\n",
    "# parser.add_argument('--st', type=str, help=\"split type\")\n",
    "# parser.add_argument('--dataset', type=str, help=\"dataset path\")\n",
    "# parser.add_argument('--wdir', type=str, help=\"directory to save weights path\")\n",
    "# parser.add_argument('--le', type=str, help=\"language embedding model\")\n",
    "# parser.add_argument('--ve', type=str, help=\"visual embedding model\")\n",
    "# parser.add_argument('--phase', type=str, help=\"train or val\")\n",
    "# parser.add_argument('--gpu', type=str, help=\"gpu device number\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "gpu = '0'\n",
    "ss = 10\n",
    "st = 'r'\n",
    "dataset_path = 'ntu_results/shift_10_r'\n",
    "# wdir = gating_our\n",
    "le = 'bert'\n",
    "ve = 'shift'\n",
    "phase = 'train'\n",
    "num_classes = 120\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir):\n",
    "#     os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir)\n",
    "# if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le):\n",
    "#     os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le)\n",
    "\n",
    "# trunk_checkpoint = '/ssd_scratch/cvit/pranay.gupta/ntu_results/vacnn_val_'+str(ss)+'_'+st+'/best_val_0.pth.tar'\n",
    "# trunk = nn.DataParallel(VA(60), device_ids=[0, 1, 2, 3])\n",
    "# trunk = trunk.to(device)\n",
    "# criterion1 = nn.CrossEntropyLoss(reduction = 'none').to(device)\n",
    "# trunk.load_state_dict(torch.load(trunk_checkpoint)['trunk_state_dict'], strict=False)\n",
    "# trunk_optimizer = optim.Adam(trunk.parameters(), lr=0.00001, weight_decay = 0.001)\n",
    "# trunk_scheduler = ReduceLROnPlateau(trunk_optimizer, mode='max', factor=0.1, patience=2, cooldown=2, verbose=True)\n",
    "# print(\"Loaded Visual Encoder\")\n",
    "\n",
    "\n",
    "criterion2 = nn.MSELoss().to(device)\n",
    "\n",
    "if ve == 'vacnn':\n",
    "    vis_emb_input_size = 2048\n",
    "elif ve == 'shift':\n",
    "    vis_emb_input_size = 256\n",
    "else: \n",
    "    pass    \n",
    "    \n",
    "text_hidden_size = 100\n",
    "vis_hidden_size = 512\n",
    "output_size = 50\n",
    "\n",
    "if le == 'bert':\n",
    "    noun_emb_input_size = 1024\n",
    "    verb_emb_input_size = 1024\n",
    "elif le == 'w2v':\n",
    "    noun_emb_input_size = 300\n",
    "    verb_emb_input_size = 300\n",
    "else:\n",
    "    pass\n",
    "\n",
    "npm_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/fgar_shift_10_r//bert/main_gvacnn_NPM.pth.tar'\n",
    "vpm_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/fgar_shift_10_r/bert/main_gvacnn_VPM.pth.tar'\n",
    "jm_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/fgar_shift_10_r/bert/main_gvacnn_JM.pth.tar'\n",
    "# npm_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/our_shift_5_n/w2v/gvacnn_NPM.pth.tar'\n",
    "# vpm_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/our_shift_5_n/w2v/gvacnn_VPM.pth.tar'\n",
    "NounPosMmen = Mmen(vis_emb_input_size, noun_emb_input_size, output_size).to(device)\n",
    "NounPosMmen.load_state_dict(torch.load(npm_checkpoint)['npm_state_dict'], strict=False)\n",
    "NounPosMmen_optimizer = optim.Adam(NounPosMmen.parameters(), lr=0.0001)\n",
    "NounPosMmen_scheduler = ReduceLROnPlateau(NounPosMmen_optimizer, mode='max', factor=0.1, patience=14, cooldown=6, verbose=True)\n",
    "\n",
    "\n",
    "VerbPosMmen = Mmen(vis_emb_input_size, verb_emb_input_size, output_size).to(device)\n",
    "VerbPosMmen.load_state_dict(torch.load(vpm_checkpoint)['vpm_state_dict'], strict=False)\n",
    "VerbPosMmen_optimizer = optim.Adam(VerbPosMmen.parameters(), lr=0.0001)\n",
    "VerbPosMmen_scheduler = ReduceLROnPlateau(VerbPosMmen_optimizer, mode='max', factor=0.1, patience=14, cooldown=6, verbose=True)\n",
    "\n",
    "# prp_emb_input_size = 300\n",
    "# PrpPosMmen = nn.DataParallel(PosMmen(vis_emb_input_size, prp_emb_input_size, hidden_size, output_size), device_ids=[0, 1, 2, 3]).to(device)\n",
    "# # VerbPosMmen.load_state_dict(torch.load(vpm_checkpoint)['vpm_state_dict'], strict=False)\n",
    "# PrpPosMmen_optimizer = optim.Adam(PrpPosMmen.parameters(), lr=0.0001)\n",
    "# PrpPosMmen_scheduler = ReduceLROnPlateau(PrpPosMmen_optimizer, mode='max', factor=0.1, patience=5, cooldown=2, verbose=True)\n",
    "\n",
    "joint_emb_size = 100\n",
    "joint_hidden_size = 50\n",
    "# # joint_out_size = 50\n",
    "JointMmen = JPosMmen(joint_emb_size, joint_hidden_size).to(device)\n",
    "JointMmen.load_state_dict(torch.load(jm_checkpoint)['vpm_state_dict'], strict=False)\n",
    "JointMmen_optimizer = optim.Adam(JointMmen.parameters(), lr=0.0001, weight_decay = 0.001)\n",
    "JointMmen_scheduler = ReduceLROnPlateau(JointMmen_optimizer, mode='max', factor=0.1, patience=14, cooldown=6, verbose=True)\n",
    "\n",
    "ntu_loaders = NTUDataLoaders(dataset_path, 'max', 1)\n",
    "train_loader = ntu_loaders.get_train_loader(1024, 8)\n",
    "zsl_loader = ntu_loaders.get_val_loader(1024, 8)\n",
    "val_loader = ntu_loaders.get_test_loader(1024, 8)\n",
    "zsl_out_loader = ntu_loaders.get_val_out_loader(1024, 8)\n",
    "val_out_loader = ntu_loaders.get_test_out_loader(1024, 8)\n",
    "train_size = ntu_loaders.get_train_size()\n",
    "zsl_size = ntu_loaders.get_val_size()\n",
    "val_size = ntu_loaders.get_test_size()\n",
    "print('Train on %d samples, validate on %d samples' % (train_size, zsl_size))\n",
    "\n",
    "\n",
    "\n",
    "nouns_vocab = np.load('nouns_vocab.npy')\n",
    "verbs_vocab = np.load('verbs_vocab.npy')\n",
    "nouns = nouns_vocab[np.argmax(np.load('nouns_ohe.npy'), -1)][:num_classes]\n",
    "# nouns[nouns == 'object'] = '#'\n",
    "# nouns[51] = 'someone'\n",
    "# nouns[55] = 'object'\n",
    "# nouns = np.load('nouns.npy')\n",
    "verbs = verbs_vocab[np.argmax(np.load('verbs_ohe.npy'), -1)][:num_classes]\n",
    "# nouns = np.load('nouns.npy')\n",
    "# verbs = np.load('verbs.npy')\n",
    "# prps = np.load('prepositions.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "if phase == 'val':\n",
    "    gzsl_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss) +'.npy')\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'v' + str(ss) + '_0.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss -ss) + '_0.npy')\n",
    "else:\n",
    "    gzsl_inds = np.arange(num_classes)\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'u' + str(ss) + '.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_classes - ss) + '.npy')\n",
    "\n",
    "unseen_labels = labels[unseen_inds]\n",
    "seen_labels = labels[seen_inds]\n",
    "\n",
    "unseen_nouns = nouns[unseen_inds]\n",
    "unseen_verbs = verbs[unseen_inds]\n",
    "# unseen_prps = prps[unseen_inds]\n",
    "seen_nouns = nouns[seen_inds]\n",
    "seen_verbs = verbs[seen_inds]\n",
    "# seen_prps = prps[seen_inds]\n",
    "verb_corp = np.unique(verbs[gzsl_inds])\n",
    "noun_corp = np.unique(nouns[gzsl_inds])\n",
    "# prp_corp = np.unique(prps[gzsl_inds])\n",
    "\n",
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('/ssd_scratch/cvit/pranay.gupta/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "# def get_w2v(model, words):\n",
    "#     emb = np.zeros([300])\n",
    "#     for word in words.split():\n",
    "#         emb += model[word]\n",
    "#     emb /= len(words.split())\n",
    "    \n",
    "#     return emb\n",
    "\n",
    "\n",
    "verb_emb = torch.from_numpy(np.load(le + '_verb.npy')[:num_classes, :]).view([num_classes, verb_emb_input_size])\n",
    "verb_emb = verb_emb/torch.norm(verb_emb, dim = 1).view([num_classes, 1]).repeat([1, verb_emb_input_size])\n",
    "noun_emb = torch.from_numpy(np.load(le + '_noun.npy')[:num_classes, :]).view([num_classes, noun_emb_input_size])\n",
    "noun_emb = noun_emb/torch.norm(noun_emb, dim = 1).view([num_classes, 1]).repeat([1, noun_emb_input_size])\n",
    "# prp_w2v = torch.from_numpy(np.array([get_w2v(model, i) for i in prps])).view([60, 300])\n",
    "# prp_w2v = noun_emb/torch.norm(prp_w2v, dim = 1).view([60, 1]).repeat([1, 300])\n",
    "\n",
    "unseen_verb_emb = verb_emb[unseen_inds, :]\n",
    "unseen_noun_emb = noun_emb[unseen_inds, :]\n",
    "# unseen_prp_w2v = prp_w2v[unseen_inds, :]\n",
    "\n",
    "seen_verb_emb = verb_emb[seen_inds, :]\n",
    "seen_noun_emb = noun_emb[seen_inds, :]\n",
    "# seen_prp_w2v = prp_w2v[seen_inds, :]\n",
    "print(\"loaded language embeddings\")\n",
    "\n",
    "\n",
    "def get_text_data(target, verb_emb, noun_emb):\n",
    "    return verb_emb[target].view(target.shape[0], verb_emb_input_size).float(), noun_emb[target].view(target.shape[0], verb_emb_input_size).float()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "def accuracy(class_embedding, vis_trans_out, target, inds):\n",
    "    inds = torch.from_numpy(inds).to(device)\n",
    "    temp_vis = vis_trans_out.unsqueeze(1).expand(vis_trans_out.shape[0], class_embedding.shape[0], vis_trans_out.shape[1])\n",
    "    temp_cemb = class_embedding.unsqueeze(0).expand(vis_trans_out.shape[0], class_embedding.shape[0], vis_trans_out.shape[1])\n",
    "    preds = torch.argmax(torch.sum(temp_vis*temp_cemb, axis=2), axis = 1)\n",
    "    acc = torch.sum(inds[preds] == target).item()/(preds.shape[0])\n",
    "    return acc, torch.sum(temp_vis*temp_cemb, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_accuracy(output, target):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(1, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    correct = correct.view(-1).float().sum(0, keepdim=True)\n",
    "    return correct.mul_(100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zsl_validate(val_loader, epoch, margin):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = AverageMeter()\n",
    "        acces = AverageMeter()\n",
    "        ce_loss_vals = []\n",
    "        ce_acc_vals = []\n",
    "        # trunk.eval()\n",
    "        NounPosMmen.eval()\n",
    "        VerbPosMmen.eval()\n",
    "        JointMmen.eval()\n",
    "        # PrpPosMmen.eval()\n",
    "        tars = []\n",
    "        preds = []\n",
    "        scores = []\n",
    "        noun_scores = []\n",
    "        verb_scores = []\n",
    "        for i, (inputs, target) in enumerate(val_loader):\n",
    "            emb = inputs.to(device)\n",
    "            # maxmin = maxmin.to(device)\n",
    "            # emb = torch.log(1 + emb)\n",
    "            verb_emb_target, noun_emb_target = get_text_data(target, verb_emb, noun_emb)\n",
    "            verb_tar = []\n",
    "            for p in verbs[target]:\n",
    "                verb_tar.append(np.argwhere(verb_corp == p)[0][0])\n",
    "\n",
    "            noun_tar = []\n",
    "            for p in np.unique(nouns[target]):\n",
    "                noun_tar.append(np.argwhere(noun_corp == p)[0][0])\n",
    "\n",
    "            # prp_tar = []\n",
    "            # for p in prps[target]:\n",
    "            #     prp_tar.append((np.argwhere(prp_corp == p)[0][0]))\n",
    "\n",
    "            # emb, output = trunk(emb, maxmin)\n",
    "            emb = emb/torch.norm(emb, dim = 1).view([emb.size(0), 1]).repeat([1, emb.shape[1]])\n",
    "            verb_embeddings = VerbPosMmen.TextArch(unseen_verb_emb.to(device).float())\n",
    "            noun_embeddings = NounPosMmen.TextArch(unseen_noun_emb.to(device).float())\n",
    "            noun_embeddings = noun_embeddings/torch.norm(noun_embeddings, dim = 1).view([noun_embeddings.size(0), 1]).repeat([1, noun_embeddings.shape[1]])\n",
    "            verb_embeddings = verb_embeddings/torch.norm(verb_embeddings, dim = 1).view([verb_embeddings.size(0), 1]).repeat([1, verb_embeddings.shape[1]])\n",
    "            # prp_embeddings = PrpPosMmen.module.TextArch(unseen_prp_w2v.to(device).float())\n",
    "            joint_text_embedding = torch.cat([verb_embeddings, noun_embeddings], axis=1)\n",
    "            # joint_text_embeddings = torch.cat([verb_embeddings, noun_embeddings], axis=1)\n",
    "            fin_text_embedding = JointMmen.TextArch(joint_text_embedding)\n",
    "            fin_text_embedding = fin_text_embedding/torch.norm(fin_text_embedding, dim = 1).view([fin_text_embedding.size(0), 1]).repeat([1, fin_text_embedding.shape[1]])\n",
    "            \n",
    "            # verb_embeddings = torch.unique(verb_embeddings, dim = 0)\n",
    "            # noun_embeddings = torch.unique(noun_embeddings, dim = 0)\n",
    "            # prp_embeddings = torch.unique(prp_embeddings, dim = 0)\n",
    "\n",
    "            vis_verb_transform, verb_transform = VerbPosMmen(emb, verb_emb_target.to(device).float())\n",
    "            vis_verb_transform = vis_verb_transform/torch.norm(vis_verb_transform, dim = 1).view([vis_verb_transform.size(0), 1]).repeat([1, vis_verb_transform.shape[1]])\n",
    "            verb_transform = verb_transform/torch.norm(verb_transform, dim = 1).view([verb_transform.size(0), 1]).repeat([1, verb_transform.shape[1]])\n",
    "#             vis_verb_reconstruction = criterion2(verb_out, verb_emb_target.to(device).float()) + criterion2(vis_verb_out, emb)\n",
    "            verb_vis_loss = multi_class_hinge_loss(vis_verb_transform, verb_transform, target, verb_embeddings, margin).to(device)\n",
    "#             verb_vis_mmd_loss = MMDLoss(vis_verb_transform, verb_transform)\n",
    "            verb_verb_loss = triplet_loss(vis_verb_transform, torch.tensor(verb_tar), device, margin).to(device)\n",
    "            \n",
    "            vis_noun_transform,  noun_transform = NounPosMmen(emb, noun_emb_target.to(device).float())\n",
    "            vis_noun_transform = vis_noun_transform/torch.norm(vis_noun_transform, dim = 1).view([vis_noun_transform.size(0), 1]).repeat([1, vis_noun_transform.shape[1]])\n",
    "            noun_transform = noun_transform/torch.norm(noun_transform, dim = 1).view([noun_transform.size(0), 1]).repeat([1, noun_transform.shape[1]])\n",
    "#             vis_noun_reconstruction = criterion2(noun_out, noun_emb_target.to(device).float()) + criterion2(vis_noun_out, emb)\n",
    "            noun_vis_loss = multi_class_hinge_loss(vis_noun_transform, noun_transform, target, noun_embeddings, margin).to(device)\n",
    "#             noun_vis_mmd_loss = MMDLoss(vis_noun_transform, noun_transform)\n",
    "            noun_noun_loss = triplet_loss(vis_noun_transform, torch.tensor(noun_tar), device, margin).to(device)\n",
    "\n",
    "            # vis_prp_out, prp_out = PrpPosMmen(emb, prp_w2v_target.to(device).float())\n",
    "            # prp_vis_loss = multi_class_hinge_loss(vis_prp_out, prp_out, target, prp_embeddings, margin).to(device)\n",
    "            # prp_prp_loss = triplet_loss(vis_prp_out, torch.tensor(prp_tar), device, margin).to(device)\n",
    "            # prp_vis_mmd_loss = MMDLoss(vis_prp_out, prp_out)\n",
    "\n",
    "            joint_vis = torch.cat([vis_verb_transform,  vis_noun_transform], axis = 1)\n",
    "            joint_text = torch.cat([verb_transform,  noun_transform], axis = 1)\n",
    "            fin_vis, fin_text = JointMmen(joint_vis, joint_text)\n",
    "            fin_text = fin_text/torch.norm(fin_text, dim = 1).view([fin_text.size(0), 1]).repeat([1, fin_text.shape[1]])\n",
    "            fin_vis = fin_vis/torch.norm(fin_vis, dim = 1).view([fin_vis.size(0), 1]).repeat([1, fin_vis.shape[1]])\n",
    "            # joint_vis = joint_vis/torch.norm(joint_vis, dim = 1).view([joint_vis.size(0), 1]).repeat([1, joint_vis.shape[1]])\n",
    "            joint_loss = multi_class_hinge_loss(fin_vis, fin_text, target, fin_text_embedding, margin).to(device)\n",
    "            \n",
    "            # joint_mmd_loss = MMDLoss(joint_vis, joint_text)\n",
    "\n",
    "            loss = verb_vis_loss + noun_vis_loss\n",
    "            # loss = noun_vis_loss\n",
    "#             loss += vis_verb_reconstruction + vis_noun_reconstruction\n",
    "            # loss += vis_noun_reconstruction     \n",
    "            loss += 0.01*(verb_verb_loss) + 0.01*(noun_noun_loss)\n",
    "            # loss += 0.01*(noun_noun_loss) \n",
    "#             loss += verb_vis_mmd_loss + noun_vis_mmd_loss\n",
    "            # loss += noun_vis_mmd_loss\n",
    "            loss += joint_loss\n",
    "            \n",
    "            # ce acc\n",
    "            ce_acc, score = accuracy(fin_text_embedding, fin_vis, target.to(device), unseen_inds)\n",
    "#             _, noun_score = accuracy(noun_embeddings, vis_noun_transform, target.to(device), unseen_inds)\n",
    "#             _, verb_score = accuracy(verb_embeddings, vis_verb_transform, target.to(device), unseen_inds)\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            acces.update(ce_acc, inputs.size(0))\n",
    "#             preds.append(pred)\n",
    "            tars.append(target)\n",
    "            scores.append(score)\n",
    "#             verb_scores.append(verb_score)\n",
    "#             noun_scores.append(noun_score)\n",
    "            ce_loss_vals.append(loss.cpu().detach().numpy())\n",
    "            ce_acc_vals.append(ce_acc)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                print('Epoch-{:<3d} {:3d}/{:3d} batches \\t'\n",
    "                    'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'accu {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n",
    "                    epoch, i + 1, len(val_loader), loss=losses, acc=acces))\n",
    "            #     print('vvrl {:.4f}\\t'\n",
    "            #           'nnrl {:.4f}\\t'.format(vis_verb_reconstruction.item(), vis_noun_reconstruction.item()))\n",
    "            #     print('vvvl {:.4f}\\t'\n",
    "            #           'nntl {:.4f}\\t'.format(verb_verb_loss.item(), noun_noun_loss.item()))\n",
    "            #     print('vtvl {:.4f}\\t'\n",
    "            #           'vtnl {:.4f}\\t'.format(verb_vis_loss.item(), noun_vis_loss.item()))\n",
    "            #     print('vvmmdl {:.4f}\\t'\n",
    "            #           'vnmmdl {:.4f}\\t'.format(verb_vis_mmd_loss.item(), noun_vis_mmd_loss.item()))\n",
    "                # print('joint loss {:.4f}'.format(joint_loss.item()))\n",
    "        return losses.avg, acces.avg, scores\n",
    "\n",
    "def validate(train_loader, epoch, margin):\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    ce_loss_vals = []\n",
    "    ce_acc_vals = []\n",
    "    scores = []\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        # (inputs, target) = next(iter(train_loader))\n",
    "        emb = inputs.to(device)\n",
    "        scores.append(emb)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0     1/ 50 batches \tloss 1.0010 (1.0010)\taccu 0.032 (0.032)\t\n",
      "Epoch-0    21/ 50 batches \tloss 0.9683 (0.9439)\taccu 0.037 (0.035)\t\n",
      "Epoch-0    41/ 50 batches \tloss 0.8269 (0.8683)\taccu 0.032 (0.040)\t\n"
     ]
    }
   ],
   "source": [
    "zsl_loss, zsl_acc, test_zs = zsl_validate(val_loader, 0, 0.3)\n",
    "test_seen = np.load('/ssd_scratch/cvit/pranay.gupta/ntu_results/shift_10_r/gtest_out.npy')\n",
    "# validate(val_out_loader, 0, 0.3)\n",
    "# zsl_loss, zsl_acc, seen_zs = zsl_validate(val_loader, visae, attae, 0)\n",
    "# val_loss, val_acc, unseen_train = validate(zsl_loader, visae, attae, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tars = []\n",
    "for i, (_, target) in enumerate(val_loader):\n",
    "    tars += list(target.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = []\n",
    "for i in tars:\n",
    "    if i in unseen_inds:\n",
    "        test_y.append(0)\n",
    "    else:\n",
    "        test_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zs = np.array([j.cpu().detach().numpy() for i in test_zs for j in i])\n",
    "# test_noun_zs = np.array([j.cpu().detach().numpy() for i in test_noun_zs for j in i])\n",
    "# test_verb_zs = np.array([j.cpu().detach().numpy() for i in test_verb_zs for j in i])\n",
    "# test_seen = np.array([j.cpu().detach().numpy() for i in test_seen for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50906, 120)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seen.shape\n",
    "# test_zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_scale(seen_features, T):\n",
    "    return np.array([np.exp(i)/np.sum(np.exp(i)) for i in (seen_features + 1e-12)/T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in test_zs])\n",
    "# prob_test_noun_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in test_noun_zs])\n",
    "# prob_test_verb_zs = np.array([np.exp(i)/np.sum(np.exp(i)) for i in test_verb_zs])\n",
    "prob_test_seen = temp_scale(test_seen, 4)\n",
    "main_prob_test_seen = np.array([np.exp(i)/np.sum(np.exp(i)) for i in test_seen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_test_zs = np.sort(prob_test_zs, 1)[:,::-1][:,:10]\n",
    "# feat_test_noun_zs = np.sort(prob_test_noun_zs, 1)[:,::-1]\n",
    "# feat_test_verb_zs = np.sort(prob_test_verb_zs, 1)[:,::-1]\n",
    "feat_test_seen = np.sort(prob_test_seen, 1)[:,::-1][:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_feat_test_zs = np.concatenate([feat_test_noun_zs, feat_test_verb_zs], 1)\n",
    "gating_test_x = np.concatenate([feat_test_zs, feat_test_seen], 1)\n",
    "gating_test_y = test_y\n",
    "# gating_train_x = np.concatenate([np.concatenate([feat_unseen_zs[train_unseen_inds, :], feat_unseen_train[train_unseen_inds, :]], 1), np.concatenate([feat_seen_zs[train_seen_inds, :], feat_seen_train[train_seen_inds, :]], 1)], 0)\n",
    "# gating_train_y = [0]*len(train_unseen_inds) + [1]*len(train_seen_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50906"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gating_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('/ssd_scratch/cvit/pranay.gupta/language_modelling/fgar_shift_10_r/gating_model_t4_thresh_0.71.pkl', 'rb') as f:\n",
    "    gating_model = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_gate = gating_model.predict_proba(gating_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = 1 - prob_gate[:, 0]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280124150394846"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_val = gating_model.predict(gating_val_x)\n",
    "np.sum(pred_test == test_y)/len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = prob_gate\n",
    "b = np.zeros(prob_gate.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gate_seen = prob_gate[:, 1]\n",
    "prob_y_given_seen = prob_test_seen + (1/120)*np.repeat((1 - p_gate_seen)[:, np.newaxis], 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gate_unseen = prob_gate[:, 0]\n",
    "prob_y_given_unseen = prob_test_zs + (1/10)*np.repeat((1 - p_gate_unseen)[:, np.newaxis], 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_seen = prob_y_given_seen*np.repeat(p_gate_seen[:, np.newaxis], 120, 1)\n",
    "prob_unseen = prob_y_given_unseen*np.repeat(p_gate_unseen[:, np.newaxis], 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "\n",
    "# soft gating\n",
    "# for i in range(len(prob_seen)):\n",
    "#     if np.max(prob_seen[i, :]) > np.max(prob_unseen[i, :]):\n",
    "#         pred = seen_inds[np.argmax(prob_seen[i, :])]\n",
    "#     else:\n",
    "#         pred = unseen_inds[np.argmax(prob_unseen[i, :])]\n",
    "#     final_preds.append(pred)\n",
    "\n",
    "# hard gating\n",
    "# final_preds = seen_inds[np.argmax(prob_test_seen, -1)]*pred_test + unseen_inds[np.argmax(prob_test_zs, -1)]*(1-pred_test)\n",
    "seen_count = 0\n",
    "tot_seen = 0\n",
    "unseen_count = 0\n",
    "tot_unseen = 0\n",
    "gseen_count = 0\n",
    "gunseen_count = 0\n",
    "for i in range(len(prob_seen)):\n",
    "    if pred_test[i] == 1:\n",
    "        pred = np.argmax(main_prob_test_seen[i, :])\n",
    "    else:\n",
    "        pred = unseen_inds[np.argmax(prob_test_zs[i, :])]\n",
    "    \n",
    "    if tars[i] in seen_inds:\n",
    "        tot_seen += 1\n",
    "        if pred_test[i] == 1:\n",
    "            gseen_count += 1\n",
    "        if pred == tars[i]:\n",
    "            seen_count += 1\n",
    "    else:\n",
    "        if pred_test[i] == 0:\n",
    "            gunseen_count+=1\n",
    "        tot_unseen += 1\n",
    "        if pred == tars[i]:\n",
    "            unseen_count += 1\n",
    "    final_preds.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4901843120445778"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gseen_count/tot_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.943711728685822"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gunseen_count/tot_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_acc = seen_count/tot_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47661808829832836"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_acc = unseen_count/tot_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46467263306641543"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mean = 2*seen_acc*unseen_acc/(seen_acc + unseen_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.470569563748726"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(933, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(pred_test == 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444533120510774"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5029498525073747"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacnn_preds = np.argmax(test_seen, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7473767691556857"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(vacnn_preds == tars)/len(tars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

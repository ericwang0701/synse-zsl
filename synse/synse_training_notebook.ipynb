{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torchvision.models as models\n",
    "from data_cnn60 import NTUDataLoaders, AverageMeter, make_dir, get_cases, get_num_classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from cada_vae import Encoder, Decoder, KL_divergence, Wasserstein_distance, reparameterize \n",
    "\n",
    "# parser = argparse.ArgumentParser(description='View adaptive')\n",
    "# parser.add_argument('--ss', type=int, help=\"split size\")\n",
    "# parser.add_argument('--st', type=str, help=\"split type\")\n",
    "# parser.add_argument('--dataset', type=str, help=\"dataset path\")\n",
    "# parser.add_argument('--wdir', type=str, help=\"directory to save weights path\")\n",
    "# parser.add_argument('--le', type=str, help=\"language embedding model\")\n",
    "# parser.add_argument('--ve', type=str, help=\"visual embedding model\")\n",
    "# parser.add_argument('--phase', type=str, help=\"train or val\")\n",
    "# parser.add_argument('--gpu', type=str, help=\"gpu device number\")\n",
    "# parser.add_argument('--ntu', type=int, help=\"ntu120 or ntu60\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "gpu = '0'\n",
    "ss = 5\n",
    "st = 'r'\n",
    "dataset_path = 'ntu_results/shift_5_r'\n",
    "wdir = 'pos_aware_cada_vae_concatenated_latent_space_shift_5_r'\n",
    "le = 'bert'\n",
    "ve = 'shift'\n",
    "phase = 'train'\n",
    "num_class = 60\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir):\n",
    "    os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir)\n",
    "if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le):\n",
    "    os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le)\n",
    "\n",
    "if ve == 'vacnn':\n",
    "    vis_emb_input_size = 2048\n",
    "elif ve == 'shift':\n",
    "    vis_emb_input_size = 256\n",
    "elif ve == 'msg3d':\n",
    "    vis_emb_input_size = 384\n",
    "else: \n",
    "    pass    \n",
    "    \n",
    "text_hidden_size = 100\n",
    "vis_hidden_size = 100\n",
    "latent_size = 100\n",
    "\n",
    "if le == 'bert':\n",
    "    text_emb_input_size = 1024\n",
    "    # verb_emb_input_size = 1024\n",
    "elif le == 'w2v':\n",
    "    text_emb_input_size = 300\n",
    "    # verb_emb_input_size = 300\n",
    "else:\n",
    "    pass\n",
    "\n",
    "sequence_encoder = Encoder([vis_emb_input_size, latent_size]).to(device)\n",
    "sequence_decoder = Decoder([latent_size, vis_emb_input_size]).to(device)\n",
    "v_text_encoder = Encoder([text_emb_input_size, latent_size//2]).to(device)\n",
    "v_text_decoder = Decoder([latent_size//2, text_emb_input_size]).to(device)\n",
    "\n",
    "n_text_encoder = Encoder([text_emb_input_size, latent_size//2]).to(device)\n",
    "n_text_decoder = Decoder([latent_size//2, text_emb_input_size]).to(device)\n",
    "\n",
    "params = []\n",
    "for model in [sequence_encoder, sequence_decoder, v_text_encoder, v_text_decoder, n_text_encoder, n_text_decoder]:\n",
    "    params += list(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params, lr = 0.0001)\n",
    "\n",
    "ntu_loaders = NTUDataLoaders(dataset_path, 'max', 1)\n",
    "train_loader = ntu_loaders.get_train_loader(64, 8)\n",
    "zsl_loader = ntu_loaders.get_val_loader(64, 8)\n",
    "val_loader = ntu_loaders.get_test_loader(64, 8)\n",
    "train_size = ntu_loaders.get_train_size()\n",
    "zsl_size = ntu_loaders.get_val_size()\n",
    "val_size = ntu_loaders.get_test_size()\n",
    "print('Train on %d samples, validate on %d samples' % (train_size, val_size))\n",
    "\n",
    "\n",
    "labels = np.load('labels.npy')\n",
    "nouns_vocab = np.load('nouns_vocab.npy')\n",
    "nouns_ohe = np.load('nouns_ohe.npy')\n",
    "verbs_vocab = np.load('verbs_vocab.npy')\n",
    "verbs_ohe = np.load('verbs_ohe.npy')\n",
    "nouns = nouns_vocab[np.argmax(nouns_ohe, -1)]\n",
    "verbs = verbs_vocab[np.argmax(verbs_ohe, -1)]\n",
    "\n",
    "if phase == 'val':\n",
    "    gzsl_inds = np.load('./label_splits/'+ st + 's' + str(num_class - ss) +'.npy')\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'v' + str(ss) + '_0.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_class -ss - ss) + '_0.npy')\n",
    "else:\n",
    "    gzsl_inds = np.arange(num_class)\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'u' + str(ss) + '.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_class  -ss) + '.npy')\n",
    "\n",
    "unseen_labels = labels[unseen_inds]\n",
    "seen_labels = labels[seen_inds]\n",
    "\n",
    "seen_verbs = verbs[seen_inds]\n",
    "unseen_verbs = verbs[unseen_inds]\n",
    "\n",
    "seen_nouns = nouns[seen_inds]\n",
    "unseen_nouns = nouns[unseen_inds]\n",
    "\n",
    "nouns_emb = torch.from_numpy(np.load(le + '_noun.npy')[:num_class,:]).view([num_class, text_emb_input_size])\n",
    "nouns_emb = nouns_emb/torch.norm(nouns_emb, dim = 1).view([num_class, 1]).repeat([1, text_emb_input_size])\n",
    "\n",
    "verbs_emb = torch.from_numpy(np.load(le + '_verb.npy')[:num_class,:]).view([num_class, text_emb_input_size])\n",
    "verbs_emb = verbs_emb/torch.norm(verbs_emb, dim = 1).view([num_class, 1]).repeat([1, text_emb_input_size])\n",
    "\n",
    "unseen_nouns_emb = nouns_emb[unseen_inds, :]\n",
    "seen_nouns_emb = nouns_emb[seen_inds, :]\n",
    "unseen_verbs_emb = verbs_emb[unseen_inds, :]\n",
    "seen_verbs_emb = verbs_emb[seen_inds, :]\n",
    "print(\"loaded language embeddings\")\n",
    "\n",
    "criterion1 = nn.MSELoss().to(device)\n",
    "\n",
    "def get_text_data(target):\n",
    "    return nouns_emb[target].view(target.shape[0], text_emb_input_size).float(), verbs_emb[target].view(target.shape[0], text_emb_input_size).float()\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_epoch = 3399\n",
    "se_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/se_'+str(load_epoch)+'.pth.tar'\n",
    "sd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/sd_'+str(load_epoch)+'.pth.tar'\n",
    "vte_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tve_'+str(load_epoch)+'.pth.tar'\n",
    "vtd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tvd_'+str(load_epoch)+'.pth.tar'\n",
    "nte_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tne_'+str(load_epoch)+'.pth.tar'\n",
    "ntd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tnd_'+str(load_epoch)+'.pth.tar'\n",
    "\n",
    "sequence_encoder.load_state_dict(torch.load(se_checkpoint)['state_dict'])\n",
    "sequence_decoder.load_state_dict(torch.load(sd_checkpoint)['state_dict'])\n",
    "v_text_encoder.load_state_dict(torch.load(vte_checkpoint)['state_dict'])\n",
    "v_text_decoder.load_state_dict(torch.load(vtd_checkpoint)['state_dict'])\n",
    "n_text_encoder.load_state_dict(torch.load(nte_checkpoint)['state_dict'])\n",
    "n_text_decoder.load_state_dict(torch.load(ntd_checkpoint)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1700, 3400):\n",
    "    losses = AverageMeter()\n",
    "    ce_loss_vals = []\n",
    "    \n",
    "    # verb models\n",
    "    sequence_encoder.train()\n",
    "    sequence_decoder.train()    \n",
    "    v_text_encoder.train()\n",
    "    v_text_decoder.train()\n",
    "    \n",
    "    # verb params\n",
    "    k_fact = max((0.1*(epoch-2700)/3000), 0)\n",
    "    cr_fact = 1*(epoch>3100)\n",
    "    v_k_fact2 = max((0.1*(epoch-3100)/3000), 0)\n",
    "    n_k_fact2 = max((0.1*(epoch-3100)/3000), 0)\n",
    "    v_cr_fact = 1*(epoch>3100)\n",
    "    n_cr_fact = 1*(epoch>3100)\n",
    "    v_lw_fact = 0\n",
    "    n_lw_fact = 0\n",
    "    \n",
    "    # noun models\n",
    "    n_text_encoder.train()\n",
    "    n_text_decoder.train()\n",
    "    \n",
    "    # nouns params\n",
    "    \n",
    "    \n",
    "    (inputs, target) = next(iter(train_loader))\n",
    "    s = inputs.to(device)\n",
    "    nt, vt = get_text_data(target)\n",
    "    nt = nt.to(device)\n",
    "    vt = vt.to(device)\n",
    "    \n",
    "    smu, slv = sequence_encoder(s)\n",
    "    sz = reparameterize(smu, slv)\n",
    "    sout = sequence_decoder(sz)\n",
    "\n",
    "    # noun forward pass\n",
    "    \n",
    "    ntmu, ntlv = n_text_encoder(nt)\n",
    "    ntz = reparameterize(ntmu, ntlv)\n",
    "    ntout = n_text_decoder(ntz)\n",
    "\n",
    "    ntfroms = n_text_decoder(sz[:,:50])\n",
    "\n",
    "    s_recons = criterion1(s, sout)\n",
    "    nt_recons = criterion1(nt, ntout)\n",
    "    s_kld = KL_divergence(smu, slv).to(device) \n",
    "    nt_kld = KL_divergence(ntmu, ntlv).to(device)\n",
    "    nt_crecons = criterion1(nt, ntfroms)\n",
    "    nl_wass = Wasserstein_distance(smu[:, :50], slv[:, :50], ntmu, ntlv)\n",
    "\n",
    "    \n",
    "    # verb forward pass\n",
    "    vtmu, vtlv = v_text_encoder(vt)\n",
    "    vtz = reparameterize(vtmu, vtlv)\n",
    "    vtout = v_text_decoder(vtz)\n",
    "\n",
    "    vtfroms = v_text_decoder(sz[:,50:])\n",
    "    vt_recons = criterion1(vt, vtout)\n",
    "    vt_kld = KL_divergence(vtmu, vtlv).to(device)\n",
    "    vt_crecons = criterion1(vt, vtfroms)\n",
    "    vl_wass = Wasserstein_distance(smu[:, 50:], slv[:, 50:], vtmu, vtlv)\n",
    "    \n",
    "    sfromt = sequence_decoder(torch.cat([ntz, vtz], 1))\n",
    "    s_crecons = criterion1(s, sfromt)\n",
    "\n",
    "    loss = s_recons + vt_recons + nt_recons \n",
    "    loss -= k_fact*(s_kld) + v_k_fact2*(vt_kld) + n_k_fact2*(nt_kld)\n",
    "    loss += n_cr_fact*(nt_crecons) + v_cr_fact*(vt_crecons) + cr_fact*(s_crecons)\n",
    "    loss += v_lw_fact*(vl_wass) + n_lw_fact*(nl_wass)\n",
    "    \n",
    "    # backward\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.update(loss.item(), inputs.size(0))\n",
    "    ce_loss_vals.append(loss.cpu().detach().numpy())\n",
    "    if epoch % 1 == 0:\n",
    "        print('---------------------')\n",
    "        print('Epoch-{:<3d} \\t'\n",
    "            'loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "            epoch, loss=losses))\n",
    "        print('srecons {:.4f}\\t ntrecons {:.4f}\\t vtrecons {:.4f}\\t'.format(s_recons.item(), nt_recons.item(), vt_recons.item()))\n",
    "        print('skld {:.4f}\\t ntkld {:.4f}\\t vtkld {:.4f}\\t'.format(s_kld.item(), nt_kld.item(), vt_kld.item()))\n",
    "        print('screcons {:.4f}\\t ntcrecons {:.4f}\\t ntcrecons {:.4f}\\t'.format(s_crecons.item(), nt_crecons.item(), vt_crecons.item()))        \n",
    "        print('nlwass {:.4f}\\t vlwass {:.4f}\\n'.format(nl_wass.item(), vl_wass.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/se_'+str(epoch)+'.pth.tar'\n",
    "sd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/sd_'+str(epoch)+'.pth.tar'\n",
    "tve_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tve_'+str(epoch)+'.pth.tar'\n",
    "tvd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tvd_'+str(epoch)+'.pth.tar'\n",
    "tne_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tne_'+str(epoch)+'.pth.tar'\n",
    "tnd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/tnd_'+str(epoch)+'.pth.tar'\n",
    "\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': sequence_encoder.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, se_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': sequence_decoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, sd_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': v_text_encoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, tve_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': v_text_decoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, tvd_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': n_text_encoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, tne_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': n_text_decoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, tnd_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cada_vae import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLP([100, 5]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_optimizer = optim.Adam(cls.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_t = unseen_nouns_emb.to(device).float()\n",
    "    n_t = n_t.repeat([500, 1])\n",
    "    v_t = unseen_verbs_emb.to(device).float()\n",
    "    v_t = v_t.repeat([500, 1])\n",
    "    y = torch.tensor(range(5)).to(device)\n",
    "    y = y.repeat([500])\n",
    "    v_text_encoder.eval()\n",
    "    n_text_encoder.eval() \n",
    "    nt_tmu, nt_tlv = n_text_encoder(n_t)\n",
    "    vt_tmu, vt_tlv = v_text_encoder(v_t)\n",
    "    vt_z = reparameterize(vt_tmu, vt_tlv)\n",
    "    nt_z = reparameterize(nt_tmu, nt_tlv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2 = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cp = []\n",
    "best = 0\n",
    "model_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/'  + wdir + '/' + le + '/cls.pth.tar'\n",
    "for c_e in range(300):\n",
    "    cls.train()\n",
    "    out = cls(torch.cat([nt_z, vt_z], 1))\n",
    "#     out = cls(vt_z)\n",
    "    c_loss = criterion2(out, y)\n",
    "    cls_optimizer.zero_grad()\n",
    "    c_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    c_acc = float(torch.sum(y == torch.argmax(out, -1)))/(5*500)\n",
    "#     cp.append(torch.argmax(out, -1))\n",
    "    print(\"Train Loss :\", c_loss.item())\n",
    "    print(\"Train Accuracy:\", c_acc)\n",
    "    cls.eval()\n",
    "#     v_out = cls(v_tmu)\n",
    "#     v_acc = float(torch.sum(v_y == torch.argmax(v_out, -1)))/500\n",
    "#     if v_acc > best:\n",
    "#         best = v_acc\n",
    "#         best_epoch = c_e\n",
    "#         save_checkpoint({ 'epoch': epoch + 1,\n",
    "#             'state_dict': cls.state_dict(),\n",
    "#         #     'optimizer': optimizer.state_dict()\n",
    "#         }, model_checkpoint)\n",
    "#         print(best_epoch)\n",
    "#     print(\"Val Accuracy:\", v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_inds = torch.from_numpy(unseen_inds)\n",
    "final_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "#     n_sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in zsl_loader:\n",
    "        t_s = inp.to(device)\n",
    "        nt_smu, t_slv = sequence_encoder(t_s)\n",
    "#         vt_smu, t_slv = v_sequence_encoder(t_s)\n",
    "        #         t_sz = reparameterize(t_smu, t_slv)\n",
    "        final_embs.append(nt_smu)\n",
    "        t_out = cls(nt_smu)\n",
    "#         t_out = cls(vt_smu)\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(unseen_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(unseen_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embs = np.array([j.cpu().numpy() for i in final_embs for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [j.item() for i in preds for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [j.item() for i in tars for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(p)\n",
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/ssd_scratch/cvit/pranay.gupta/synse_5_r_embedding.npy', final_embs)\n",
    "np.save('/ssd_scratch/cvit/pranay.gupta/synse_5_r_gt.npy', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in val_loader:\n",
    "        t_s = inp.to(device)\n",
    "        t_smu, t_slv = sequence_encoder(t_s)\n",
    "#         t_sz = reparameterize(t_smu, t_slv)\n",
    "#         final_embs.append(t_smu)\n",
    "        t_out = cls(t_smu)\n",
    "        val_out_embs.append(F.softmax(t_out))\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(unseen_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(unseen_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_embs = np.array([j.cpu().numpy() for i in val_out_embs for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/ssd_scratch/cvit/pranay.gupta/unseen_out/synse_5_r_gzsl_zs.npy', val_out_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(t, p)\n",
    "unseen_acc = 0\n",
    "# seen_acc = 0\n",
    "for i, val in enumerate(unseen_inds.numpy()):\n",
    "    unseen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    print(labels[val], ' : ', cmat[i, i]/np.sum(cmat[i]))\n",
    "    print(labels[unseen_inds.numpy()[np.argsort(cmat[i])[::-1]]])\n",
    "    print(np.sort(cmat[i])[::-1])\n",
    "\n",
    "# for i in seen_inds:\n",
    "#     seen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    \n",
    "unseen_acc = unseen_acc/ss\n",
    "# seen_acc = seen_acc/(60-ss)\n",
    "# h_mean = 2*unseen_acc*seen_acc/(unseen_acc+ seen_acc)\n",
    "print('\\n')\n",
    "print('unseen_class_accuracy : ', unseen_acc)\n",
    "# print('seen_class_accuacy : ',  seen_acc)\n",
    "# print('harmonic_mean : ', h_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cada_vae import MLP\n",
    "cls = MLP([100, 60]).to(device)\n",
    "cls_optimizer = optim.Adam(cls.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_feats = {}\n",
    "for num, (inp, target) in enumerate(train_loader):\n",
    "    for i, label in enumerate(target):\n",
    "        if label.item() not in seen_feats:\n",
    "            seen_feats[label.item()] = inp[i, :].view(1, 256)\n",
    "        else:\n",
    "            seen_feats[label.item()] = torch.cat([seen_feats[label.item()], inp[i,:].view(1, 256)], 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_t = unseen_nouns_emb.to(device).float()\n",
    "    n_t = n_t.repeat([500, 1])\n",
    "    v_t = unseen_verbs_emb.to(device).float()\n",
    "    v_t = v_t.repeat([500, 1])\n",
    "    y = torch.tensor(range(5)).to(device)\n",
    "    y = y.repeat([500])\n",
    "    \n",
    "    for i, l in enumerate(seen_feats):\n",
    "        if i == 0:\n",
    "            s_t = seen_feats[l][sorted(np.random.choice(seen_feats[l].shape[0], 200, replace = False)), :]\n",
    "            y_s = [l]*200\n",
    "        else:\n",
    "            s_t = np.vstack([s_t, seen_feats[l][sorted(np.random.choice(seen_feats[l].shape[0], 200, replace = False)), :]])\n",
    "            y_s += [l]*200\n",
    "            \n",
    "    s_t = torch.from_numpy(s_t).to(device)\n",
    "    y_s = torch.tensor(y_s).to(device)\n",
    "    v_text_encoder.eval()\n",
    "    n_text_encoder.eval() \n",
    "    nt_tmu, nt_tlv = n_text_encoder(n_t)\n",
    "    vt_tmu, vt_tlv = v_text_encoder(v_t)\n",
    "    vt_z = reparameterize(vt_tmu, vt_tlv)\n",
    "    nt_z = reparameterize(nt_tmu, nt_tlv)\n",
    "    \n",
    "    s_tmu, s_tlv = sequence_encoder(s_t)\n",
    "    s_z = reparameterize(s_tmu, s_tlv)\n",
    "    \n",
    "    f_z = torch.cat([torch.cat([nt_z, vt_z], 1), s_z], 0)\n",
    "    f_y = torch.cat([y, y_s], 0)\n",
    "#     v_t = unseen_labels_emb.to(device).repeat([100, 1])\n",
    "#     v_y = torch.tensor(range(5)).to(device).repeat([100])\n",
    "#     v_tmu, v_tlv = text_encoder(v_t)\n",
    "\n",
    "criterion2 = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = []\n",
    "best = 0\n",
    "model_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/'  + wdir + '/' + le + '/cls.pth.tar'\n",
    "for c_e in range(2000):\n",
    "    cls.train()\n",
    "    out = cls(f_z)\n",
    "    c_loss = criterion2(out, f_y)\n",
    "    cls_optimizer.zero_grad()\n",
    "    c_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    c_acc = float(torch.sum(f_y == torch.argmax(out, -1)))/13000\n",
    "#     cp.append(torch.argmax(out, -1))\n",
    "    print(\"Train Loss :\", c_loss.item())\n",
    "    print(\"Train Accuracy:\", c_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzsl_inds = torch.from_numpy(gzsl_inds)\n",
    "final_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in val_loader:\n",
    "        t_s = inp.to(device)\n",
    "        t_smu, t_slv = sequence_encoder(t_s)\n",
    "#         t_sz = reparameterize(t_smu, t_slv)\n",
    "        final_embs.append(t_smu)\n",
    "        t_out = cls(t_smu)\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(gzsl_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(gzsl_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embs = np.array([j.cpu().numpy() for i in final_embs for j in i])\n",
    "p = [j.item() for i in preds for j in i]\n",
    "t = [j.item() for i in tars for j in i]\n",
    "p = np.array(p)\n",
    "t = np.array(t)\n",
    "\n",
    "cmat = confusion_matrix(t, p)\n",
    "unseen_acc = 0\n",
    "seen_acc = 0\n",
    "for i, val in enumerate(unseen_inds):\n",
    "    unseen_acc += cmat[val, val]/np.sum(cmat[val])\n",
    "    print(labels[val], ' : ', cmat[val, val]/np.sum(cmat[val]))\n",
    "    print(labels[gzsl_inds.numpy()[np.argsort(cmat[val])[::-1]]])\n",
    "    print(np.sort(cmat[val])[::-1])\n",
    "\n",
    "for i in seen_inds:\n",
    "    seen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    \n",
    "unseen_acc = unseen_acc/ss\n",
    "seen_acc = seen_acc/(60-ss)\n",
    "h_mean = 2*unseen_acc*seen_acc/(unseen_acc+ seen_acc)\n",
    "print('\\n')\n",
    "print('unseen_class_accuracy : ', unseen_acc)\n",
    "print('seen_class_accuacy : ',  seen_acc)\n",
    "print('harmonic_mean : ', h_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plot = []\n",
    "for i in t:\n",
    "    t_plot.append(np.argwhere(unseen_inds == i).flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=1500)\n",
    "tsne_results = tsne.fit_transform(final_embs)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = t_plot, cmap='Dark2')\n",
    "\n",
    "for i in range(5):\n",
    "    plt.annotate(labels[t[i]], (tsne_results[i, 0], tsne_results[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(tz.detach().cpu().numpy()[inds, :])\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = target.detach().cpu().numpy()[inds], cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(sout.detach().cpu().numpy())\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = target.detach().cpu().numpy(), cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sout[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max((s[0] - sout[0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(torch.mean(smu, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(slv, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sigma = torch.exp(0.5*slv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.FloatTensor(sigma.size()[0], 1).normal_(0, 1).expand(sigma.size()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_test = eps*sigma + smu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = []\n",
    "for num, t in enumerate(target):\n",
    "    if t == 0:\n",
    "        ind.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smu[ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(tmu, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(tlv, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

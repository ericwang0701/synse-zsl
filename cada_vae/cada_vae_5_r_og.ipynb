{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import os.path as osp\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torchvision.models as models\n",
    "from resnext_specialist import VA\n",
    "from data_cnn60 import NTUDataLoaders, AverageMeter, make_dir, get_cases, get_num_classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from cada_vae import Encoder, Decoder, KL_divergence, Wasserstein_distance, reparameterize, triplet_loss\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='View adaptive')\n",
    "# parser.add_argument('--ss', type=int, help=\"split size\")\n",
    "# parser.add_argument('--st', type=str, help=\"split type\")\n",
    "# parser.add_argument('--dataset', type=str, help=\"dataset path\")\n",
    "# parser.add_argument('--wdir', type=str, help=\"directory to save weights path\")\n",
    "# parser.add_argument('--le', type=str, help=\"language embedding model\")\n",
    "# parser.add_argument('--ve', type=str, help=\"visual embedding model\")\n",
    "# parser.add_argument('--phase', type=str, help=\"train or val\")\n",
    "# parser.add_argument('--gpu', type=str, help=\"gpu device number\")\n",
    "# parser.add_argument('--ntu', type=int, help=\"ntu120 or ntu60\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "gpu = '0'\n",
    "ss = 5\n",
    "st = 'r'\n",
    "dataset_path = 'ntu_results/shift_5_r'\n",
    "wdir = 'cada_vae_shift_5_r'\n",
    "le = 'bert'\n",
    "ve = 'shift'\n",
    "phase = 'train'\n",
    "num_class = 60\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "seed = 5\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir):\n",
    "    os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir)\n",
    "if not os.path.exists('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le):\n",
    "    os.mkdir('/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le)\n",
    "\n",
    "if ve == 'vacnn':\n",
    "    vis_emb_input_size = 2048\n",
    "elif ve == 'shift':\n",
    "    vis_emb_input_size = 256\n",
    "elif ve == 'msg3d':\n",
    "    vis_emb_input_size = 384\n",
    "else: \n",
    "    pass    \n",
    "    \n",
    "text_hidden_size = 100\n",
    "vis_hidden_size = 100\n",
    "latent_size = 50\n",
    "\n",
    "if le == 'bert':\n",
    "    text_emb_input_size = 1024\n",
    "    # verb_emb_input_size = 1024\n",
    "elif le == 'w2v':\n",
    "    text_emb_input_size = 300\n",
    "    # verb_emb_input_size = 300\n",
    "else:\n",
    "    pass\n",
    "\n",
    "sequence_encoder = Encoder([vis_emb_input_size, vis_hidden_size, latent_size]).to(device)\n",
    "sequence_decoder = Decoder([latent_size, vis_hidden_size, vis_emb_input_size]).to(device)\n",
    "text_encoder = Encoder([text_emb_input_size, latent_size]).to(device)\n",
    "text_decoder = Decoder([latent_size, text_emb_input_size]).to(device)\n",
    "\n",
    "params = []\n",
    "for model in [sequence_encoder, sequence_decoder, text_encoder, text_decoder]:\n",
    "    params += list(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(params, lr = 0.0001)\n",
    "# NounPosMmen_scheduler = ReduceLROnPlateau(NounPosMmen_optimizer, mode='max', factor=0.1, patience=14, cooldown=6, verbose=True)\n",
    "\n",
    "ntu_loaders = NTUDataLoaders(dataset_path, 'max', 1)\n",
    "train_loader = ntu_loaders.get_train_loader(64, 8)\n",
    "zsl_loader = ntu_loaders.get_val_loader(64, 8)\n",
    "val_loader = ntu_loaders.get_test_loader(64, 8)\n",
    "train_size = ntu_loaders.get_train_size()\n",
    "zsl_size = ntu_loaders.get_val_size()\n",
    "val_size = ntu_loaders.get_test_size()\n",
    "print('Train on %d samples, validate on %d samples' % (train_size, val_size))\n",
    "\n",
    "\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "if phase == 'val':\n",
    "    gzsl_inds = np.load('./label_splits/'+ st + 's' + str(num_class - ss) +'.npy')\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'v' + str(ss) + '_0.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_class -ss - ss) + '_0.npy')\n",
    "else:\n",
    "    gzsl_inds = np.arange(num_class)\n",
    "    unseen_inds = np.sort(np.load('./label_splits/' + st + 'u' + str(ss) + '.npy'))\n",
    "    seen_inds = np.load('./label_splits/'+ st + 's' + str(num_class  -ss) + '.npy')\n",
    "\n",
    "unseen_labels = labels[unseen_inds]\n",
    "seen_labels = labels[seen_inds]\n",
    "\n",
    "labels_emb = torch.from_numpy(np.load(le + '_labels.npy')[:num_class,:]).view([num_class, text_emb_input_size])\n",
    "labels_emb = labels_emb/torch.norm(labels_emb, dim = 1).view([num_class, 1]).repeat([1, text_emb_input_size])\n",
    "\n",
    "unseen_labels_emb = labels_emb[unseen_inds, :]\n",
    "seen_labels_emb = labels_emb[seen_inds, :]\n",
    "print(\"loaded language embeddings\")\n",
    "\n",
    "criterion1 = nn.MSELoss().to(device)\n",
    "\n",
    "def get_text_data(target, labels_emb):\n",
    "    return labels_emb[target].view(target.shape[0], text_emb_input_size).float()\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar', is_best=False):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_epoch = 8499\n",
    "se_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/se_trip'+str(load_epoch)+'.pth.tar'\n",
    "sd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/sd_trip'+str(load_epoch)+'.pth.tar'\n",
    "te_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/te_trip'+str(load_epoch)+'.pth.tar'\n",
    "td_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/td_trip'+str(load_epoch)+'.pth.tar'\n",
    "\n",
    "sequence_encoder.load_state_dict(torch.load(se_checkpoint)['state_dict'])\n",
    "sequence_decoder.load_state_dict(torch.load(sd_checkpoint)['state_dict'])\n",
    "text_encoder.load_state_dict(torch.load(te_checkpoint)['state_dict'])\n",
    "text_decoder.load_state_dict(torch.load(td_checkpoint)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(8500, 10200):\n",
    "    losses = AverageMeter()\n",
    "    ce_loss_vals = []\n",
    "    sequence_encoder.train()\n",
    "    sequence_decoder.train()    \n",
    "    text_encoder.train()\n",
    "    text_decoder.train()\n",
    "    k_trip = 0\n",
    "    k_fact = max((0.1*(epoch-9500)/3000), 0)\n",
    "#     k_fact2 = max((0.1*(epoch-3100)/3000), 0)\n",
    "    k_fact2 = k_fact*(epoch>9900)\n",
    "    cr_fact = 1*(epoch>9900)\n",
    "    lw_fact = 0\n",
    "    (inputs, target) = next(iter(train_loader))\n",
    "    s = inputs.to(device)\n",
    "    t = get_text_data(target, labels_emb).to(device)\n",
    "    smu, slv = sequence_encoder(s)\n",
    "    sz = reparameterize(smu, slv)\n",
    "    sout = sequence_decoder(sz)\n",
    "\n",
    "    tmu, tlv = text_encoder(t)\n",
    "    tz = reparameterize(tmu, tlv)\n",
    "    tout = text_decoder(tz)\n",
    "\n",
    "    # cross reconstruction\n",
    "    tfroms = text_decoder(sz)\n",
    "    sfromt = sequence_decoder(tz)\n",
    "\n",
    "    s_triplet = triplet_loss(smu, target, device)\n",
    "#     t_triplet = triplet_loss(tmu, target, device)\n",
    "    s_recons = criterion1(s, sout)\n",
    "    t_recons = criterion1(t, tout)\n",
    "    s_kld = KL_divergence(smu, slv).to(device) \n",
    "    t_kld = KL_divergence(tmu, tlv).to(device)\n",
    "    s_crecons = criterion1(s, sfromt)\n",
    "    t_crecons = criterion1(t, tfroms)\n",
    "    l_wass = Wasserstein_distance(smu, slv, tmu, tlv)\n",
    "    \n",
    "\n",
    "    loss = s_recons + t_recons \n",
    "    loss += k_trip*s_triplet\n",
    "    loss -= k_fact*(s_kld)\n",
    "    loss -= k_fact2*(t_kld)\n",
    "    loss += cr_fact*(s_crecons + t_crecons)\n",
    "    loss += lw_fact*(l_wass)\n",
    "\n",
    "    # backward\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.update(loss.item(), inputs.size(0))\n",
    "    ce_loss_vals.append(loss.cpu().detach().numpy())\n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch-{:<3d} \\t'\n",
    "            'loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "            epoch, loss=losses))\n",
    "        print('srecons {:.4f}\\ttrecons {:.4f}\\t'.format(s_recons.item(), t_recons.item()))\n",
    "        print('skld {:.4f}\\ttkld {:.4f}\\t'.format(s_kld.item(), t_kld.item()))\n",
    "        print('screcons {:.4f}\\ttcrecons {:.4f}\\t'.format(s_crecons.item(), t_crecons.item()))        \n",
    "        print('lwass {:.4f}\\t'.format(l_wass.item()))\n",
    "        print('strip {:.4f}\\t'.format(s_triplet.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/se_trip'+str(epoch)+'.pth.tar'\n",
    "sd_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/sd_trip'+str(epoch)+'.pth.tar'\n",
    "te_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/te_trip'+str(epoch)+'.pth.tar'\n",
    "td_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/' + wdir + '/' + le + '/td_trip'+str(epoch)+'.pth.tar'\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': sequence_encoder.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, se_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': sequence_decoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, sd_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': text_encoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, te_checkpoint)\n",
    "save_checkpoint({ 'epoch': epoch + 1,\n",
    "    'state_dict': text_decoder.state_dict(),\n",
    "#     'optimizer': optimizer.state_dict()\n",
    "}, td_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = smu.detach().cpu().numpy()\n",
    "b = slv.detach().cpu().numpy()\n",
    "c = tmu.detach().cpu().numpy()\n",
    "d = tlv.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cada_vae import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = MLP([50, 5]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_optimizer = optim.Adam(cls.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    c_t = unseen_labels_emb.to(device)\n",
    "    c_t = c_t.repeat([500, 1])\n",
    "    y = torch.tensor(range(5)).to(device)\n",
    "    y = y.repeat([500])\n",
    "    text_encoder.eval()\n",
    "    t_tmu, t_tlv = text_encoder(c_t)\n",
    "    t_z = reparameterize(t_tmu, t_tlv)\n",
    "    v_t = unseen_labels_emb.to(device).repeat([100, 1])\n",
    "    v_y = torch.tensor(range(5)).to(device).repeat([100])\n",
    "    v_tmu, v_tlv = text_encoder(v_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2 = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cp = []\n",
    "best = 0\n",
    "model_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/'  + wdir + '/' + le + '/cls.pth.tar'\n",
    "for c_e in range(300):\n",
    "    cls.train()\n",
    "    out = cls(t_z)\n",
    "    c_loss = criterion2(out, y)\n",
    "    cls_optimizer.zero_grad()\n",
    "    c_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    c_acc = float(torch.sum(y == torch.argmax(out, -1)))/2500\n",
    "#     cp.append(torch.argmax(out, -1))\n",
    "    print(\"Train Loss :\", c_loss.item())\n",
    "    print(\"Train Accuracy:\", c_acc)\n",
    "    cls.eval()\n",
    "    v_out = cls(v_tmu)\n",
    "    v_acc = float(torch.sum(v_y == torch.argmax(v_out, -1)))/500\n",
    "    if v_acc > best:\n",
    "        best = v_acc\n",
    "        best_epoch = c_e\n",
    "#         save_checkpoint({ 'epoch': epoch + 1,\n",
    "#             'state_dict': cls.state_dict(),\n",
    "#         #     'optimizer': optimizer.state_dict()\n",
    "#         }, model_checkpoint)\n",
    "        print(best_epoch)\n",
    "    print(\"Val Accuracy:\", v_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_inds = torch.from_numpy(unseen_inds)\n",
    "final_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in zsl_loader:\n",
    "        t_s = inp.to(device)\n",
    "        t_smu, t_slv = sequence_encoder(t_s)\n",
    "#         t_sz = reparameterize(t_smu, t_slv)\n",
    "        final_embs.append(t_smu)\n",
    "        t_out = cls(t_smu)\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(unseen_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(unseen_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embs = np.array([j.cpu().numpy() for i in final_embs for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [j.item() for i in preds for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [j.item() for i in tars for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(p)\n",
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/ssd_scratch/cvit/pranay.gupta/umap_embeddings/cadavae_5_r_embedding.npy', final_embs)\n",
    "np.save('/ssd_scratch/cvit/pranay.gupta/umap_embeddings/cadavae_5_r_gt.npy', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in val_loader:\n",
    "        t_s = inp.to(device)\n",
    "        t_smu, t_slv = sequence_encoder(t_s)\n",
    "#         t_sz = reparameterize(t_smu, t_slv)\n",
    "#         final_embs.append(t_smu)\n",
    "        t_out = cls(t_smu)\n",
    "        val_out_embs.append(F.softmax(t_out))\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(unseen_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(unseen_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_embs = np.array([j.cpu().numpy() for i in val_out_embs for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_out_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/ssd_scratch/cvit/pranay.gupta/unseen_out/cadavae_5_r_gzsl_zs.npy', val_out_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = confusion_matrix(t, p)\n",
    "unseen_acc = 0\n",
    "# seen_acc = 0\n",
    "for i, val in enumerate(unseen_inds.numpy()):\n",
    "    unseen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    print(labels[val], ' : ', cmat[i, i]/np.sum(cmat[i]))\n",
    "    print(labels[unseen_inds.numpy()[np.argsort(cmat[i])[::-1]]])\n",
    "    print(np.sort(cmat[i])[::-1])\n",
    "\n",
    "# for i in seen_inds:\n",
    "#     seen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    \n",
    "unseen_acc = unseen_acc/ss\n",
    "# seen_acc = seen_acc/(60-ss)\n",
    "# h_mean = 2*unseen_acc*seen_acc/(unseen_acc+ seen_acc)\n",
    "print('\\n')\n",
    "print('unseen_class_accuracy : ', unseen_acc)\n",
    "# print('seen_class_accuacy : ',  seen_acc)\n",
    "# print('harmonic_mean : ', h_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cada_vae import MLP\n",
    "cls = MLP([50, 60]).to(device)\n",
    "cls_optimizer = optim.Adam(cls.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_feats = {}\n",
    "for num, (inp, target) in enumerate(train_loader):\n",
    "    for i, label in enumerate(target):\n",
    "        if label.item() not in seen_feats:\n",
    "            seen_feats[label.item()] = inp[i, :].view(1, 256)\n",
    "        else:\n",
    "            seen_feats[label.item()] = torch.cat([seen_feats[label.item()], inp[i,:].view(1, 256)], 0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c_t = unseen_labels_emb.to(device)\n",
    "    c_t = c_t.repeat([500, 1])\n",
    "    \n",
    "    y = torch.tensor(range(5)).to(device)\n",
    "    y = y.repeat([500])\n",
    "    \n",
    "    for i, l in enumerate(seen_feats):\n",
    "        if i == 0:\n",
    "            s_t = seen_feats[l][sorted(np.random.choice(seen_feats[l].shape[0], 200, replace = False)), :]\n",
    "            y_s = [l]*200\n",
    "        else:\n",
    "            s_t = np.vstack([s_t, seen_feats[l][sorted(np.random.choice(seen_feats[l].shape[0], 200, replace = False)), :]])\n",
    "            y_s += [l]*200\n",
    "            \n",
    "    s_t = torch.from_numpy(s_t).to(device)\n",
    "    y_s = torch.tensor(y_s).to(device)\n",
    "    text_encoder.eval()\n",
    "    sequence_encoder.eval()\n",
    "    t_tmu, t_tlv = text_encoder(c_t)\n",
    "    t_z = reparameterize(t_tmu, t_tlv)\n",
    "    \n",
    "    s_tmu, s_tlv = sequence_encoder(s_t)\n",
    "    s_z = reparameterize(s_tmu, s_tlv)\n",
    "    \n",
    "    f_z = torch.cat([t_z, s_z], 0)\n",
    "    f_y = torch.cat([y, y_s], 0)\n",
    "#     v_t = unseen_labels_emb.to(device).repeat([100, 1])\n",
    "#     v_y = torch.tensor(range(5)).to(device).repeat([100])\n",
    "#     v_tmu, v_tlv = text_encoder(v_t)\n",
    "\n",
    "criterion2 = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = []\n",
    "best = 0\n",
    "model_checkpoint = '/ssd_scratch/cvit/pranay.gupta/language_modelling/'  + wdir + '/' + le + '/cls.pth.tar'\n",
    "for c_e in range(2000):\n",
    "    cls.train()\n",
    "    out = cls(f_z)\n",
    "    c_loss = criterion2(out, f_y)\n",
    "    cls_optimizer.zero_grad()\n",
    "    c_loss.backward()\n",
    "    cls_optimizer.step()\n",
    "    c_acc = float(torch.sum(f_y == torch.argmax(out, -1)))/13000\n",
    "#     cp.append(torch.argmax(out, -1))\n",
    "    print(\"Train Loss :\", c_loss.item())\n",
    "    print(\"Train Accuracy:\", c_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gzsl_inds = torch.from_numpy(gzsl_inds)\n",
    "final_embs = []\n",
    "with torch.no_grad():\n",
    "    sequence_encoder.eval()\n",
    "    cls.eval()\n",
    "    count = 0\n",
    "    num = 0\n",
    "    preds = []\n",
    "    tars = []\n",
    "    for (inp, target) in val_loader:\n",
    "        t_s = inp.to(device)\n",
    "        t_smu, t_slv = sequence_encoder(t_s)\n",
    "#         t_sz = reparameterize(t_smu, t_slv)\n",
    "        final_embs.append(t_smu)\n",
    "        t_out = cls(t_smu)\n",
    "        pred = torch.argmax(t_out, -1)\n",
    "        preds.append(gzsl_inds[pred])\n",
    "        tars.append(target)\n",
    "        count += torch.sum(gzsl_inds[pred] == target)\n",
    "        num += len(target)\n",
    "    print(float(count)/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_embs = np.array([j.cpu().numpy() for i in final_embs for j in i])\n",
    "p = [j.item() for i in preds for j in i]\n",
    "t = [j.item() for i in tars for j in i]\n",
    "p = np.array(p)\n",
    "t = np.array(t)\n",
    "\n",
    "cmat = confusion_matrix(t, p)\n",
    "unseen_acc = 0\n",
    "seen_acc = 0\n",
    "for i, val in enumerate(unseen_inds):\n",
    "    unseen_acc += cmat[val, val]/np.sum(cmat[val])\n",
    "    print(labels[val], ' : ', cmat[val, val]/np.sum(cmat[val]))\n",
    "    print(labels[gzsl_inds.numpy()[np.argsort(cmat[val])[::-1]]])\n",
    "    print(np.sort(cmat[val])[::-1])\n",
    "\n",
    "for i in seen_inds:\n",
    "    seen_acc += cmat[i, i]/np.sum(cmat[i])\n",
    "    \n",
    "unseen_acc = unseen_acc/ss\n",
    "seen_acc = seen_acc/(60-ss)\n",
    "h_mean = 2*unseen_acc*seen_acc/(unseen_acc+ seen_acc)\n",
    "print('\\n')\n",
    "print('unseen_class_accuracy : ', unseen_acc)\n",
    "print('seen_class_accuacy : ',  seen_acc)\n",
    "print('harmonic_mean : ', h_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_plot = []\n",
    "for i in t:\n",
    "    t_plot.append(np.argwhere(unseen_inds == i).flatten()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=1500)\n",
    "tsne_results = tsne.fit_transform(final_embs)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = t_plot, cmap='Dark2')\n",
    "\n",
    "for i in range(5):\n",
    "    plt.annotate(labels[t[i]], (tsne_results[i, 0], tsne_results[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(tz.detach().cpu().numpy()[inds, :])\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = target.detach().cpu().numpy()[inds], cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=500)\n",
    "tsne_results = tsne.fit_transform(sout.detach().cpu().numpy())\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tsne_results[:,0], tsne_results[:,1], c = target.detach().cpu().numpy(), cmap='plasma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sout[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max((s[0] - sout[0])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(torch.mean(smu, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(slv, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " sigma = torch.exp(0.5*slv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = torch.FloatTensor(sigma.size()[0], 1).normal_(0, 1).expand(sigma.size()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_test = eps*sigma + smu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = []\n",
    "for num, t in enumerate(target):\n",
    "    if t == 0:\n",
    "        ind.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smu[ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(tmu, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(torch.mean(tlv, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = cdist(unseen_labels_emb, unseen_labels_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_class_embedding = t_z[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = cdist(latent_class_embedding.cpu(), latent_class_embedding.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_mu = t_tmu[:5].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_lv = t_tlv[:5].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdist(latent_mu, latent_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdist(latent_lv, latent_lv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
